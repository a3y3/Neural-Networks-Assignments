\documentclass[12pt]{article}
\usepackage{fullpage,enumerate,amsmath,amssymb,graphicx}
\begin{document}

\begin{center}
{\Large ICT 4012 Spring 2017 Homework [3]} %For example: Homework 1

\begin{tabular}{rl}
MU Registration No.: & [140911090] \\  % Enter your MU Registration 
Name: & [Soham Dongargaonkar] \\   % Enter your full name
%Collaborators: & [list all the people you worked with (if permitted)] % Only if permitted by the teacher, other just comment this line
\end{tabular}
\end{center}

%==============Do not change this statement===================================================================
By turning in this assignment, I agree by the academic honor code and declare that all of this is my own work.
%=============================================================================================================

\section*{Problem 1}

\begin{enumerate}
    \item \begin{gather}
                \phi(a) =  \frac{1}{1+e^{-av}} \nonumber\\ \nonumber\\
                \frac{d}{dv}\phi(a) = \frac{d}{dv}(\frac{1}{1+e^{-av}})\nonumber\\\nonumber\\
                \frac{d}{dv}\phi(a) = \frac{ae^{av}}{(1+e^{-av})^2} \label{eqn:1}
            \end{gather}
            \[ 
                \text{[Substituting\ v = 0]} 
            \]
             
            \begin{gather}
                \phi'(v) = \frac{a}{4}
            \end{gather}  
               

   
\end{enumerate}

\section*{Problem 2}

\begin{enumerate}
  \item \[
            \text{In this question, we must vary a and see how the function changes values.}
        \]
        \[
            \text{Hence, a is not a constant anymore. We assume v as constant in this proof.}
        \]
        \begin{gather}
            \therefore\phi(a) = \tanh(av) \nonumber \\ \nonumber\\ 
            \lim_{a\to\infty} \phi(a) = \lim_{a\to\infty}\tanh(av) \label{eqn:2}
        \end{gather}
        
        \[
            \text{But\ $tanh(av)$ =$\frac{e^{av}-e^{-av}}{e^{av}+e^{-av}}$} \\
        \]
        
        \begin{gather}             
            \therefore \lim_{a\to\infty} \phi(a) = \lim_{a\to\infty}\frac{e^{av}-e^{-av}}{e^{av}+e^{-av}} \label{eqn:3}    
        \end{gather}
        \[
            \text{For $\lim_{a\to\infty}$, take $\frac{e^{av}-e^{-av}}{e^{av}+e^{-av}}$=$\frac{1-e^{-2ah}}{1+e^{-2ah}}$}
        \]
        \[
            \text{And for $\lim_{a\to-\infty}$, take $\frac{e^{av}-e^{-av}}{e^{av}+e^{-av}}$=$\frac{e^{2ah}-1}{e^{2ah}+1}$}
        \]
        
        \[
            \text{Applying appropriate limits, for $\lim_{a\to\infty}$, we get} 
        \]
        \begin{gather}
            \frac{1-0}{1+0} = 1
        \end{gather}
        
        \[
            \text{And for $\lim_{a\to-\infty}$, we get} 
        \]
        \begin{gather}
            \frac{0-1}{0+1} = -1
        \end{gather}
       
        \[
            \text{Hence, we prove that as slope parameter a approaches $\infty$, } 
        \]
        \[
            \text{the function starts acting like a signum function.}
        \]
\end{enumerate}

\section*{Problem 3}
\begin{align*}
    1.\text{Hard Limit}: &y = \begin{cases}
                            0,  v<0 \\
                            1,  v>= 0
                        \end{cases}\\\\
    2.\text{Symmetrical Hard Limit}: &y = \begin{cases}
                            -1,  v<0 \\
                            1,  v>= 0
                        \end{cases}\\\\ 
    3.\text{Linear}: &y = v\\\\
    4.\text{Saturating Linear}: &y = \begin{cases}
                            0,  v<0 \\
                            n,  0<=v<1\\
                            1,  v>1
                        \end{cases}\\\\
    5.\text{Log-Sigmoid: }&y =\frac{1}{1+e^{-v}} \\\\
    6.\text{Positive Linear}: &y = \begin{cases}
                            1,  v<0 \\
                            v,  v>= 0
                        \end{cases}\\\\
\end{align*}

\section*{Problem 4}
\begin{gather}
    \text{Starting at the output neurons f and g, we have}\\
    y_f=\phi({\sum{w_{fi}*x_i}}) \\
    \text{ie. Assuming $x_i$ is output of previous neuron}\\
    \text{and}\\
    y_g=\phi({\sum{w_{gi}*x_i}})\\
    \text{Error $e_j$ is calculated as $d_j-y_j$ respectively, for both of them.}\\
    \delta_f = e_j*y_j*[1-y_j]\\
    \Delta w_{fd} = -\eta \delta_f y_d\\
    \Delta w_{gd} = -\eta \delta_g y_d\\
    \text{}\\
    \text{Now, moving back to the 2nd hidden layer consisting nodes d and e}\\
    \delta_d = y_d*[1-y_d]*\sum{\delta_f*w_fd}\\
    \text{and}\\
    \delta_e = y_e*[1-y_e]*\sum{\delta_g*w_gd}\\
    \\\text{Similarly, the algorithm continues till first input layer.}
\end{gather}
\end{document}
