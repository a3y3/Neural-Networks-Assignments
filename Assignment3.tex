\documentclass[12pt]{article}
\usepackage{fullpage,enumerate,amsmath,amssymb,graphicx}
\begin{document}

\begin{center}
{\Large ICT 4012 Spring 2017 Homework [3]} %For example: Homework 1

\begin{tabular}{rl}
MU Registration No.: & [140911090] \\  % Enter your MU Registration 
Name: & [Soham Dongargaonkar] \\   % Enter your full name
%Collaborators: & [list all the people you worked with (if permitted)] % Only if permitted by the teacher, other just comment this line
\end{tabular}
\end{center}

%==============Do not change this statement===================================================================
By turning in this assignment, I agree by the academic honor code and declare that all of this is my own work.
%=============================================================================================================

\section*{Problem 1}

\begin{enumerate}
    \item \begin{gather}
                \phi(a) =  \frac{1}{1+e^{-av}} \nonumber\\ \nonumber\\
                \frac{d}{dv}\phi(a) = \frac{d}{dv}(\frac{1}{1+e^{-av}})\nonumber\\\nonumber\\
                \frac{d}{dv}\phi(a) = \frac{ae^{av}}{(1+e^{-av})^2} \label{eqn:1}
            \end{gather}
            \[ 
                \text{[Substituting\ v = 0]} 
            \]
             
            \begin{gather}
                \phi'(v) = \frac{a}{4}
            \end{gather}  
               

   
\end{enumerate}

\section*{Problem 2}

\begin{gather*}
  \text{The limitation of the Guass-Newton method is that it follows only a forward pass approach}\\
  \text{and does not keep updating the synaptic weights. We can implement a backward pass system}\\
  \text{that keeps updating weights of the current layer with respect to the next layer.}\\
  \text{A typical example in which this can be implemented is the Back Propagation Algorithm.}
\end{gather*}

\section*{Problem 3}
\begin{gather*}
    \Delta w_{ji}(n) = \alpha \Delta w_{ji}(n-1) + \eta\delta_j(n)y_i(n)\\\\
    \text{Put n=2}\\
\end{gather*}
\begin{align*}
    \Delta w_{ji}(2) &= \alpha \Delta w_{ji}(1) + \eta\delta_j(2)y_i(2)\\\\
    &=\alpha[\alpha\Delta w_{ji}(0)+\eta\delta_j(1)y_i(1)]+ \eta\delta_j(2)y_i(2)\\\\
    &=\alpha^2\Delta w_{ji}(0)+\alpha\eta\Delta w_j(1)y_i(1)+ \eta\delta_j(2)y_i(2)\\ \\
    &=\alpha^2[\alpha\Delta_{ji}(-1)+\eta\delta_j(0)y_i(0)]+\alpha\eta\Delta w_j(1)y_i(1)+ \eta\delta_j(2)y_i(2)\\ \\
    &=\alpha^2\eta\delta_j(0)y_i(0) +\alpha\eta\Delta w_j(1)y_i(1)+ \eta\delta_j(2)y_i(2)\\ 
\end{align*}
\[  
    \text{The above equation can also be generalized in the following way}\\
\]
\begin{gather*}
    \Delta w_{ji}(n) = \eta\sum_{t=0}^{N}\alpha^{n-t}\delta_j(t)y_i(t)
\end{gather*}

\section*{Problem 4}
\begin{gather*}
    \text{Starting at the output neurons f and g, we have}\\
    y_f=\phi({\sum{w_{fi}*x_i}}) \\
    \text{ie. Assuming $x_i$ is output of previous neuron}\\
    \text{and}\\
    y_g=\phi({\sum{w_{gi}*x_i}})\\
    \text{Error $e_j$ is calculated as $d_j-y_j$ respectively, for both of them.}\\
    \delta_f = e_j*y_j*[1-y_j]\\
    \Delta w_{fd} = -\eta \delta_f y_d\\
    \Delta w_{gd} = -\eta \delta_g y_d\\
    \text{}\\
    \text{Now, moving back to the 2nd hidden layer consisting nodes d and e}\\
    \delta_d = y_d*[1-y_d]*\sum{\delta_f*w_fd}\\
    \text{and}\\
    \delta_e = y_e*[1-y_e]*\sum{\delta_g*w_gd}\\
    \\\text{Similarly, the algorithm continues till first input layer.}
\end{gather*}

\section*{Problem 5}
\begin{gather*}
    X = \left[
                {\begin{array}{ccc}
                     1&2&3  \\
                     4&5&6  \\
                     7&8&9
                \end{array}} \right
             ]\\
    \text{Then, pseudoinverse of $X = {[X^T.X]}^{-1}X^T$}\\\\
    \text{Now,}\\
    X^T.X = \left[
                {\begin{array}{ccc}
                     66&78&90  \\
                     78&93&108  \\
                     90&108&126
                \end{array}} \right
             ]\\
    \text{which is a non invertible matrix.}\\\\
    \text{Value of}
    X.X^T = \left[
                {\begin{array}{ccc}
                     14&32&50  \\
                     32&77&122  \\
                     50&122&194
                \end{array}} \right
             ]\\
    \text{which is again a singular matrix.}\\\\
    \text{Hence, the pseudo inverse of X cannot be found using Moore-Penrose formula.}
\end{gather*}
\end{document}
